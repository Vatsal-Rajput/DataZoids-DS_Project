---
title: "Report"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
library(readxl)
library(dplyr)
library(ggfortify)
data <- read_excel("../Data/MarketingData.xlsx")
```

## Principal Component Analysis

The data we have was high dimensional data with multiple columns which led to difficulty in organizing and analyzing the various components. We thus used PCA to reduce the dimensionalty of our data.

First step was to clean and filter out the data which was not required. We removed the categorical variables as they were increasing the complexity of the data. Further, we removed the columns with zero variance and the rows with missing values. Following is the head of the cleaned data:

```{r}
#| echo: false
categorical_columns <- data %>%
  select(where(is.character))
numerical_columns <- data %>%
  select(-all_of(colnames(categorical_columns)))
required_columns <- numerical_columns[, sapply(numerical_columns, function(col) var(col, na.rm = TRUE) != 0)] 
required_columns <- na.omit(required_columns)  

print(head(required_columns, n=5))
```

As we can see, there are 24 columns even after cleaning the data, which shows the high dimensionalty.

Then, we performed PCA which gave the following result:

```{r}
#| echo: false
pc <- prcomp(required_columns, scale. = TRUE) 
summary(pc)  
```

Below is the plot of the PCA analysis and the cumulative variance of the components:

```{r}
#| echo: false

plot(pc)

```

```{r}
#| echo: false

plot(cumsum(pc$sdev^2/sum(pc$sdev^2)))

```

Thus, around 13 components are able to explain 80% variability in the data.

Below is a plot of the relationship between the first two components after PCA.

```{r}
#| echo: false

autoplot(pc, data=required_columns)

```

Thus, with the help of PCA we can reduce the data with 24 columns to upto 13 columns and still explain 80% variability in the data.
